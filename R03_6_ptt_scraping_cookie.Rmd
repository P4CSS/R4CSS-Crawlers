---
title: "R03_5_PTT_scraping_cookie"
author: "Jilung Hsieh"
date: "2019/9/2"
output:
  html_document:
    highlight: zenburn
    number_sections: yes
    theme: cerulean
    toc: yes
    css: style.css
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# loading packages
```{r}
library(rvest)
library(httr)
library(tidyverse)
options(stringsAsFactors = F)
```

# GET() html with cookie

## Testing: GET() directly
```{r}
# url
url <- "https://www.ptt.cc/bbs/HatePolitics/search?page=1&q=%E6%9E%97%E6%98%B6%E4%BD%90"

# Using read_html(), write_html() and browseURL() to examine the link
read_html(url) %>% write_html("test.html")

# Browsing the URL by browseURL()
browseURL("test.html")
```



## Testing: GET() with cookie
```{r}
# GET html with cookie
response <- GET(url, config = set_cookies("over18" = "1"))

# content() %>% read_html() to an xml_document
response %>% 
    content("text") %>%
    read_html() %>%
    write_html("test_cookie.html")
# Examining the url
browseURL("test_cookie.html")
```

## Code: GET() html with cookie
```{r}
url <- "https://www.ptt.cc/bbs/HatePolitics/search?page=1&q=%E6%9E%97%E6%98%B6%E4%BD%90"

# GET() with cookie and convert to xml_document by read_html()
doc <- GET(url, config = set_cookies("over18" = "1")) %>%
    content("text") %>%
    read_html()

# write_html() again to final checking
write_html(doc, "test_cookie.html")
browseURL("test_cookie.html")
```


# Parse html

```{r}
# GET() all nodes
nodes <- doc %>% html_nodes(".r-ent")

# For all nodes, retrieve number of recommendation to var nrec
nrec <- nodes %>% html_node(".nrec span") %>% html_text() %>% as.numeric()

# For all nodes, retrieve title to variable title
title <- nodes %>% html_node(".title a") %>% html_text()

# For all nodes, retrieve link to variable link
# Remember to paste the prefix link to the link
# Try to browse it for verification
pre <- "https://www.ptt.cc"
link <- nodes %>% 
    html_node(".title a") %>% 
    html_attr("href") %>%
    str_c(pre, .)
link[1] %>% browseURL()

# For all nodes, retrieve author to variable author
author <- nodes %>%
    html_node(".meta .author") %>%
    html_text()
author

# Combine all variable as data.frame
page.df <- data_frame(nrec, title, link, author)
```



# Formatting the url
```{r}
query = "林昶佐"
pre <- "https://www.ptt.cc"
url <- str_c("https://www.ptt.cc/bbs/HatePolitics/search?page=", 1, "&q=", query)
url
```


# Using for-loop to get back all pages
```{r}
query = "林昶佐"
post.df <- data_frame()
for(page in 1:8){
    url <- str_c("https://www.ptt.cc/bbs/HatePolitics/search?page=", page, "&q=", query)
    print(url)
    doc <- GET(url, config = set_cookies("over18" = "1")) %>%
        content("text") %>%
        read_html()
    nodes <- doc %>% html_nodes(".r-ent")
    nrec <- nodes %>% html_node(".nrec span") %>% html_text() %>% as.numeric()
    title <- nodes %>% html_node(".title a") %>% html_text()
    link <- nodes %>% html_node(".title a") %>%  html_attr("href") %>%
        str_c(pre, .)
    author <- nodes %>% html_node(".meta .author") %>% html_text()
    page.df <- data_frame(nrec, title, link, author)
    
    post.df <- bind_rows(post.df, page.df)
    print(nrow(post.df))
}
```


# NOTES and FURTHER
Now we detect the last page number manually. You can try to write a function to crawl back all data given a board name and a query. One more thing you need to think by yourself is that you need to detect the last page number automatically. Try to do it!


